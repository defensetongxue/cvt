{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from numpy import repeat\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import paddle\n",
    "import numpy as np\n",
    "\n",
    "def PaddleRearrange(tensor:paddle.Tensor, pattern: str, **axes_lengths) -> paddle.Tensor:\n",
    "    x=np.array(tensor)\n",
    "    return paddle.to_tensor(rearrange(x,pattern,**axes_lengths))\n",
    "class RearrangeLayer(nn.layer):\n",
    "    def forword(sef,x:paddle.Tensor,pattern: str, **axes_lengths):\n",
    "        return PaddleRearrange(x,pattern,**axes_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# From PyTorch internals\n",
    "\"\"\"对repeat进行封装，让代码更加健壮\"\"\"\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, Iterable):#如果已经是转换后的值，直接返回，不需要再做转换操作\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "\n",
    "    return parse\n",
    "to_1tuple = _ntuple(1)\n",
    "to_2tuple = _ntuple(2)\n",
    "to_3tuple = _ntuple(3)\n",
    "to_4tuple = _ntuple(4)\n",
    "to_ntuple = _ntuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理fp16(16位小数),按照fp32进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: paddle.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(paddle.float32))\n",
    "        return ret.type(orig_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重写GELU函数，降低处理精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickGELU(nn.Layer):\n",
    "    def forward(self, x: paddle.Tensor):\n",
    "        return x * paddle.sigmoid(1.702 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全连接网络，复用自 paddle vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Layer):\n",
    "    \"\"\" MLP module\n",
    "    Impl using nn.Linear and activation is GELU, dropout is applied.\n",
    "    Ops: fc -> act -> dropout -> fc -> dropout\n",
    "    Attributes:\n",
    "        fc1: nn.Linear\n",
    "        fc2: nn.Linear\n",
    "        act: GELU\n",
    "        dropout1: dropout after fc1\n",
    "        dropout2: dropout after fc2\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 mlp_ratio,\n",
    "                 dropout=0.):\n",
    "        super().__init__()\n",
    "        w_attr_1, b_attr_1 = self._init_weights()\n",
    "        self.fc1 = nn.Linear(embed_dim,\n",
    "                             int(embed_dim * mlp_ratio),\n",
    "                             weight_attr=w_attr_1,\n",
    "                             bias_attr=b_attr_1)\n",
    "\n",
    "        w_attr_2, b_attr_2 = self._init_weights()\n",
    "        self.fc2 = nn.Linear(int(embed_dim * mlp_ratio),\n",
    "                             embed_dim,\n",
    "                             weight_attr=w_attr_2,\n",
    "                             bias_attr=b_attr_2)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        weight_attr = paddle.ParamAttr(\n",
    "            initializer=paddle.nn.initializer.XavierUniform()) #default in pp: xavier\n",
    "        bias_attr = paddle.ParamAttr(\n",
    "            initializer=paddle.nn.initializer.Normal(std=1e-6)) #default in pp: zero\n",
    "        return weight_attr, bias_attr\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEmbed(nn.Layer):\n",
    "    \"\"\" Image to Conv Embedding\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 patch_size=7,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=64,\n",
    "                 stride=4,\n",
    "                 padding=2,\n",
    "                 norm_layer=None):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)#把patch初始化为一个正方形,这里是(7,7)\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2D(\n",
    "            in_chans, embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.proj(x)\n",
    "        print(x.shape)\n",
    "        B, C, H, W = x.shape#B个图片H*W的大小 C个通道(example：W==3:红黄蓝)\n",
    "        x = PaddleRearrange(x, 'b c h w -> b (h w) c')#对每个图片进行嵌入，相当于对每个图片线性的堆叠\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        x = PaddleRearrange(x, 'b (h w) c -> b c h w', h=H, w=W)#把x回归原来的形状\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_in,\n",
    "                 dim_out,\n",
    "                 num_heads,\n",
    "                 qkv_bias=False,\n",
    "                 attn_drop=0.,\n",
    "                 proj_drop=0.,\n",
    "                 method='dw_bn',\n",
    "                 kernel_size=3,\n",
    "                 stride_kv=1,\n",
    "                 stride_q=1,\n",
    "                 padding_kv=1,\n",
    "                 padding_q=1,\n",
    "                 with_cls_token=True,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.stride_kv = stride_kv\n",
    "        self.stride_q = stride_q\n",
    "        self.dim = dim_out\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = dim_out ** -0.5\n",
    "        self.with_cls_token = with_cls_token\n",
    "\n",
    "        # calculate q,k,v with conv\n",
    "\n",
    "        self.conv_proj_q = self._build_projection(\n",
    "            dim_in, dim_out, kernel_size, padding_q,\n",
    "            stride_q, 'linear' if method == 'avg' else method\n",
    "        )\n",
    "        self.conv_proj_k = self._build_projection(\n",
    "            dim_in, dim_out, kernel_size, padding_kv,\n",
    "            stride_kv, method\n",
    "        )\n",
    "        self.conv_proj_v = self._build_projection(\n",
    "            dim_in, dim_out, kernel_size, padding_kv,\n",
    "            stride_kv, method\n",
    "        )\n",
    "\n",
    "        # init parameters of q,k,v\n",
    "\n",
    "        self.proj_q = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "        self.proj_k = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "        self.proj_v = nn.Linear(dim_in, dim_out, bias=qkv_bias)\n",
    "\n",
    "        # init project other parameters\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim_out, dim_out)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def _build_projection(self,\n",
    "                          dim_in,\n",
    "                          dim_out,\n",
    "                          kernel_size,\n",
    "                          padding,\n",
    "                          stride,\n",
    "                          method):\n",
    "        if method == 'dw_bn':\n",
    "            proj = nn.Sequential(OrderedDict([\n",
    "                ('conv', nn.Conv2D(\n",
    "                    dim_in,\n",
    "                    dim_in,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                    groups=dim_in\n",
    "                )),\n",
    "                ('bn', nn.BatchNorm2d(dim_in)),\n",
    "                ('rearrage', RearrangeLayer('b c h w -> b (h w) c')),\n",
    "            ]))\n",
    "        elif method == 'avg':\n",
    "            proj = nn.Sequential(OrderedDict([\n",
    "                ('avg', nn.AvgPool2D(\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    stride=stride,\n",
    "                    ceil_mode=True\n",
    "                )),\n",
    "                ('rearrage', RearrangeLayer('b c h w -> b (h w) c')),\n",
    "            ]))\n",
    "        elif method == 'linear':\n",
    "            proj = None\n",
    "        else:\n",
    "            raise ValueError('Unknown method ({})'.format(method))\n",
    "\n",
    "        return proj\n",
    "\n",
    "    def forward_conv(self, x, h, w):\n",
    "        if self.with_cls_token:\n",
    "            cls_token, x = paddle.split(x, [1, h*w], 1)\n",
    "\n",
    "        x =  PaddleRearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "\n",
    "        if self.conv_proj_q is not None:\n",
    "            q = self.conv_proj_q(x)\n",
    "        else:\n",
    "            q =  PaddleRearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "        if self.conv_proj_k is not None:\n",
    "            k = self.conv_proj_k(x)\n",
    "        else:\n",
    "            k =  PaddleRearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "        if self.conv_proj_v is not None:\n",
    "            v = self.conv_proj_v(x)\n",
    "        else:\n",
    "            v =  PaddleRearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "        if self.with_cls_token:\n",
    "            q = paddle.cat((cls_token, q), dim=1)\n",
    "            k = paddle.cat((cls_token, k), dim=1)\n",
    "            v = paddle.cat((cls_token, v), dim=1)\n",
    "\n",
    "        return q, k, v\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        if (\n",
    "            self.conv_proj_q is not None\n",
    "            or self.conv_proj_k is not None\n",
    "            or self.conv_proj_v is not None\n",
    "        ):\n",
    "            q, k, v = self.forward_conv(x, h, w)\n",
    "\n",
    "        q =  PaddleRearrange(self.proj_q(q), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        k =  PaddleRearrange(self.proj_k(k), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "        v =  PaddleRearrange(self.proj_v(v), 'b t (h d) -> b h t d', h=self.num_heads)\n",
    "\n",
    "        attn_score = paddle.einsum('bhlk,bhtk->bhlt', [q, k]) * self.scale\n",
    "        attn = paddle.nn.functional.softmax(attn_score, dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = paddle.einsum('bhlt,bhtv->bhlv', [attn, v])\n",
    "        x =  PaddleRearrange(x, 'b h t d -> b t (h d)')\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_macs(module, input, output):\n",
    "        # T: num_token\n",
    "        # S: num_token\n",
    "        input = input[0]\n",
    "        flops = 0\n",
    "\n",
    "        _, T, C = input.shape\n",
    "        H = W = int(np.sqrt(T-1)) if module.with_cls_token else int(np.sqrt(T))\n",
    "\n",
    "        H_Q = H / module.stride_q\n",
    "        W_Q = H / module.stride_q\n",
    "        T_Q = H_Q * W_Q + 1 if module.with_cls_token else H_Q * W_Q\n",
    "\n",
    "        H_KV = H / module.stride_kv\n",
    "        W_KV = W / module.stride_kv\n",
    "        T_KV = H_KV * W_KV + 1 if module.with_cls_token else H_KV * W_KV\n",
    "\n",
    "        # C = module.dim\n",
    "        # S = T\n",
    "        # Scaled-dot-product macs\n",
    "        # [B x T x C] x [B x C x T] --> [B x T x S]\n",
    "        # multiplication-addition is counted as 1 because operations can be fused\n",
    "        flops += T_Q * T_KV * module.dim\n",
    "        # [B x T x S] x [B x S x C] --> [B x T x C]\n",
    "        flops += T_Q * module.dim * T_KV\n",
    "\n",
    "        if (\n",
    "            hasattr(module, 'conv_proj_q')\n",
    "            and hasattr(module.conv_proj_q, 'conv')\n",
    "        ):\n",
    "            params = sum(\n",
    "                [\n",
    "                    p.numel()\n",
    "                    for p in module.conv_proj_q.conv.parameters()\n",
    "                ]\n",
    "            )\n",
    "            flops += params * H_Q * W_Q\n",
    "\n",
    "        if (\n",
    "            hasattr(module, 'conv_proj_k')\n",
    "            and hasattr(module.conv_proj_k, 'conv')\n",
    "        ):\n",
    "            params = sum(\n",
    "                [\n",
    "                    p.numel()\n",
    "                    for p in module.conv_proj_k.conv.parameters()\n",
    "                ]\n",
    "            )\n",
    "            flops += params * H_KV * W_KV\n",
    "\n",
    "        if (\n",
    "            hasattr(module, 'conv_proj_v')\n",
    "            and hasattr(module.conv_proj_v, 'conv')\n",
    "        ):\n",
    "            params = sum(\n",
    "                [\n",
    "                    p.numel()\n",
    "                    for p in module.conv_proj_v.conv.parameters()\n",
    "                ]\n",
    "            )\n",
    "            flops += params * H_KV * W_KV\n",
    "\n",
    "        params = sum([p.numel() for p in module.proj_q.parameters()])\n",
    "        flops += params * T_Q\n",
    "        params = sum([p.numel() for p in module.proj_k.parameters()])\n",
    "        flops += params * T_KV\n",
    "        params = sum([p.numel() for p in module.proj_v.parameters()])\n",
    "        flops += params * T_KV\n",
    "        params = sum([p.numel() for p in module.proj.parameters()])\n",
    "        flops += params * T\n",
    "\n",
    "        module.__flops__ += flops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim_in,\n",
    "                 dim_out,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.with_cls_token = kwargs['with_cls_token']\n",
    "\n",
    "        self.norm1 = norm_layer(dim_in)\n",
    "        self.attn = Attention(\n",
    "            dim_in, dim_out, num_heads, qkv_bias, attn_drop, drop,\n",
    "            **kwargs\n",
    "        )\n",
    "        if drop_path>0. :\n",
    "            self.drop_path=nn.Dropout(drop_path)\n",
    "        else:\n",
    "            self.drop_path=nn.Identity()\n",
    "        #self.drop_path = DropPath(drop_path) \\\n",
    "        #    if drop_path > 0. else nn.Identity()\n",
    "        \n",
    "        self.norm2 = norm_layer(dim_out)\n",
    "\n",
    "        dim_mlp_hidden = int(dim_out * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim_out,\n",
    "            hidden_features=dim_mlp_hidden,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        res = x\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        attn = self.attn(x, h, w)\n",
    "        x = res + self.drop_path(attn)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "632b173250d16d6f9b5bd55f86a0e0c96b37a21df53bd488615da0bb1f86a5c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('cvt': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
