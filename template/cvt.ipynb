{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_dict_by_key(cond, d):\n",
    "    '''\n",
    "    cond: function\n",
    "    d: dic()\n",
    "    divide the d:dic() in to two parts,[0] is kwargs with with matching func(cond),[1]  else(not match)\n",
    "    '''\n",
    "    return_val = [dict(), dict()]\n",
    "    for key in d.keys():\n",
    "        match = bool(cond(key))\n",
    "        ind = int(not match)\n",
    "        return_val[ind][key] = d[key]\n",
    "    return (*return_val,)\n",
    "\n",
    "def group_by_key_prefix_and_remove_prefix(prefix, d):\n",
    "    '''\n",
    "    prefix: string\n",
    "    d: dict()\n",
    "    for i in d:\n",
    "        if i start with <prefix> :\n",
    "            remove <prefix> ,take it in kwargs_without_prefix:dic()\n",
    "        else:\n",
    "            take it in kwargs:dic()\n",
    "    '''\n",
    "    kwargs_with_prefix, kwargs = group_dict_by_key(lambda x: x.startswith(prefix), d)\n",
    "    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n",
    "    return kwargs_without_prefix, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module): \n",
    "    ''' \n",
    "    may equal to torch.nn.BatchNorm2d (https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html?highlight=batchnorm2d#torch.nn.BatchNorm2d)\n",
    "    '''\n",
    "    def __init__(self, dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        std = torch.var(x, dim = 1, unbiased = False, keepdim = True).sqrt()\n",
    "        mean = torch.mean(x, dim = 1, keepdim = True)\n",
    "        return (x - mean) / (std + self.eps) * self.g + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    '''\n",
    "    add a layer fn(norm(x)), normalize x before sending it to fn\n",
    "    dim:number number of dimension\n",
    "    fn:a alayer of a network\n",
    "    '''\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    ''' \n",
    "    dim:number number of dimension\n",
    "    mult out_channels/in_channels =mult\n",
    "    dropout: Randomly discarded\n",
    "    '''\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim * mult, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(dim * mult, dim, 1),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthWiseConv2d(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, kernel_size, padding, stride, bias = True):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_in, kernel_size = kernel_size, padding = padding, groups = dim_in, stride = stride, bias = bias),\n",
    "            nn.BatchNorm2d(dim_in),\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size = 1, bias = bias)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, proj_kernel, kv_proj_stride, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        padding = proj_kernel // 2\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "\n",
    "        self.to_q = DepthWiseConv2d(dim, inner_dim, proj_kernel, padding = padding, stride = 1, bias = False)\n",
    "        self.to_kv = DepthWiseConv2d(dim, inner_dim * 2, proj_kernel, padding = padding, stride = kv_proj_stride, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(inner_dim, dim, 1),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        b, n, _, y, h = *shape, self.heads\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = 1))#cal q,k,v from x\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h d) x y -> (b h) (x y) d', h = h), (q, k, v))\n",
    "\n",
    "        dots = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, y = y)\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, proj_kernel, kv_proj_stride, depth, heads, dim_head = 64, mlp_mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, proj_kernel = proj_kernel, kv_proj_stride = kv_proj_stride, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_mult, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CvT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        num_classes,\n",
    "        s1_emb_dim = 64,\n",
    "        s1_emb_kernel = 7,\n",
    "        s1_emb_stride = 4,\n",
    "        s1_proj_kernel = 3,\n",
    "        s1_kv_proj_stride = 2,\n",
    "        s1_heads = 1,\n",
    "        s1_depth = 1,\n",
    "        s1_mlp_mult = 4,\n",
    "        s2_emb_dim = 192,\n",
    "        s2_emb_kernel = 3,\n",
    "        s2_emb_stride = 2,\n",
    "        s2_proj_kernel = 3,\n",
    "        s2_kv_proj_stride = 2,\n",
    "        s2_heads = 3,\n",
    "        s2_depth = 2,\n",
    "        s2_mlp_mult = 4,\n",
    "        s3_emb_dim = 384,\n",
    "        s3_emb_kernel = 3,\n",
    "        s3_emb_stride = 2,\n",
    "        s3_proj_kernel = 3,\n",
    "        s3_kv_proj_stride = 2,\n",
    "        s3_heads = 6,\n",
    "        s3_depth = 10,\n",
    "        s3_mlp_mult = 4,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        kwargs = dict(locals()) #get local param in kwargs\n",
    "\n",
    "        dim = 3 \n",
    "        layers = [] \n",
    "\n",
    "        for prefix in ('s1', 's2', 's3'):\n",
    "            config, kwargs = group_by_key_prefix_and_remove_prefix(f'{prefix}_', kwargs)#get kwargs starting with s1|s2|s3\n",
    "            \n",
    "            #build 3 layers network conv(卷积),normlayer(神经网络)，transfromer，\n",
    "            layers.append(nn.Sequential(\n",
    "                nn.Conv2d(dim, config['emb_dim'], kernel_size = config['emb_kernel'], padding = (config['emb_kernel'] // 2), stride = config['emb_stride']),\n",
    "                LayerNorm(config['emb_dim']),\n",
    "                Transformer(dim = config['emb_dim'], proj_kernel = config['proj_kernel'], kv_proj_stride = config['kv_proj_stride'], depth = config['depth'], heads = config['heads'], mlp_mult = config['mlp_mult'], dropout = dropout)\n",
    "            ))\n",
    "\n",
    "            dim = config['emb_dim']\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            *layers,\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            Rearrange('... () () -> ...'),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "632b173250d16d6f9b5bd55f86a0e0c96b37a21df53bd488615da0bb1f86a5c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('cvt': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
