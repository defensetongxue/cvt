## 同样的输出到一个同样的层，经过同样的weight和bias。完全不一样

```python
class LayerNorm(nn.LayerNorm):
    """Subclass torch's LayerNorm to handle fp16."""

    def forward(self, x: paddle.Tensor):
        orig_type = x.dtype
        ret = super().forward(x.type(paddle.float32))
        return ret.type(orig_type)

····
self.norm = norm_layer(dim_embed)#norm_layer=LayerNorm
····
#一样的输入
x = self.norm(cls_tokens)
#不一样的输出
print(self.norm.weight[:3])
print(self.norm.bias[:3])
```
torch
```python
torch
tensor([1., 1., 1.], grad_fn=<SliceBackward0>)
tensor([0., 0., 0.], grad_fn=<SliceBackward0>)
```
paddle
```python
Tensor(shape=[3], dtype=float32, place=CPUPlace, stop_gradient=False,
       [1., 1., 1.])
Tensor(shape=[3], dtype=float32, place=CPUPlace, stop_gradient=False,
       [0., 0., 0.])
```

## 多出了.num_batches_tracked，paddle没有这个weight，详细打印在`weight/paddle.txt` 和`weight/torch.txt`

其实感觉没什么印象，是用来计数的一个weight